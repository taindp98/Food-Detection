{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/facebookresearch/detr.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "# from tqdm.autonotebook import tqdm\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import collections\n",
    "\n",
    "#Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "\n",
    "#sklearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "#CV\n",
    "import cv2\n",
    "\n",
    "#Glob\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if '../../albumentations' not in sys.path:\n",
    "    sys.path.append('../../albumentations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "from albumentations.pytorch.transforms import ToTensorV2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Albumenatations\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ################# DETR FUCNTIONS FOR LOSS######################## \n",
    "# import sys\n",
    "# sys.path.append('./detr/')\n",
    "\n",
    "# from detr.models.matcher import HungarianMatcher\n",
    "# from detr.models.detr import SetCriterion\n",
    "# #################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reading annotations.json\n",
    "TRAIN_ANNOTATIONS_PATH = \"../resource/train/ann_train_without_error.json\"\n",
    "TRAIN_IMAGE_DIRECTIORY = \"D:\\\\data\\\\food_detection\\\\public_train\\\\images\"\n",
    "\n",
    "VAL_ANNOTATIONS_PATH = \"../resource/val/ann_valid.json\"\n",
    "VAL_IMAGE_DIRECTIORY = \"D:\\\\data\\\\food_detection\\\\public_validation\\\\images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CLASSES = ('bread-wholemeal', 'jam', 'water', 'bread-sourdough', 'banana', 'soft-cheese', 'ham-raw', 'hard-cheese', 'cottage-cheese', 'bread-half-white', 'coffee-with-caffeine', 'fruit-salad', 'pancakes', 'tea', 'salmon-smoked', 'avocado', 'spring-onion-scallion', 'ristretto-with-caffeine', 'ham', 'egg', 'bacon-frying', 'chips-french-fries', 'juice-apple', 'chicken', 'tomato-raw', 'broccoli', 'shrimp-boiled', 'beetroot-steamed-without-addition-of-salt', 'carrot-raw', 'chickpeas', 'french-salad-dressing', 'pasta-hornli', 'sauce-cream', 'meat-balls', 'pasta', 'tomato-sauce', 'cheese', 'pear', 'cashew-nut', 'almonds', 'lentils', 'mixed-vegetables', 'peanut-butter', 'apple', 'blueberries', 'cucumber', 'cocoa-powder', 'greek-yaourt-yahourt-yogourt-ou-yoghourt', 'maple-syrup-concentrate', 'buckwheat-grain-peeled', 'butter', 'herbal-tea', 'mayonnaise', 'soup-vegetable', 'wine-red', 'wine-white', 'green-bean-steamed-without-addition-of-salt', 'sausage', 'pizza-margherita-baked', 'salami', 'mushroom', 'bread-meat-substitute-lettuce-sauce', 'tart', 'tea-verveine', 'rice', 'white-coffee-with-caffeine', 'linseeds', 'sunflower-seeds', 'ham-cooked', 'bell-pepper-red-raw', 'zucchini', 'green-asparagus', 'tartar-sauce', 'lye-pretzel-soft', 'cucumber-pickled', 'curry-vegetarian', 'yaourt-yahourt-yogourt-ou-yoghourt-natural', 'soup-of-lentils-dahl-dhal', 'soup-cream-of-vegetables', 'balsamic-vinegar', 'salmon', 'salt-cake-vegetables-filled', 'bacon', 'orange', 'pasta-noodles', 'cream', 'cake-chocolate', 'pasta-spaghetti', 'black-olives', 'parmesan', 'spaetzle', 'salad-lambs-ear', 'salad-leaf-salad-green', 'potatoes-steamed', 'white-cabbage', 'halloumi', 'beetroot-raw', 'bread-grain', 'applesauce-unsweetened-canned', 'cheese-for-raclette', 'mushrooms', 'bread-white', 'curds-natural-with-at-most-10-fidm', 'bagel-without-filling', 'quiche-with-cheese-baked-with-puff-pastry', 'soup-potato', 'bouillon-vegetable', 'beef-sirloin-steak', 'taboule-prepared-with-couscous', 'eggplant', 'bread', 'turnover-with-meat-small-meat-pie-empanadas', 'mungbean-sprouts', 'mozzarella', 'pasta-penne', 'lasagne-vegetable-prepared', 'mandarine', 'kiwi', 'french-beans', 'tartar-meat', 'spring-roll-fried', 'pork-chop', 'caprese-salad-tomato-mozzarella', 'leaf-spinach', 'roll-of-half-white-or-white-flour-with-large-void', 'pasta-ravioli-stuffing', 'omelette-plain', 'tuna', 'dark-chocolate', 'sauce-savoury', 'dried-raisins', 'ice-tea', 'kaki', 'macaroon', 'smoothie', 'crepe-plain', 'chicken-nuggets', 'chili-con-carne-prepared', 'veggie-burger', 'cream-spinach', 'cod', 'chinese-cabbage', 'hamburger-bread-meat-ketchup', 'soup-pumpkin', 'sushi', 'chestnuts', 'coffee-decaffeinated', 'sauce-soya', 'balsamic-salad-dressing', 'pasta-twist', 'bolognaise-sauce', 'leek', 'fajita-bread-only', 'potato-gnocchi', 'beef-cut-into-stripes-only-meat', 'rice-noodles-vermicelli', 'tea-ginger', 'tea-green', 'bread-whole-wheat', 'onion', 'garlic', 'hummus', 'pizza-with-vegetables-baked', 'beer', 'glucose-drink-50g', 'chicken-wing', 'ratatouille', 'peanut', 'high-protein-pasta-made-of-lentils-peas', 'cauliflower', 'quiche-with-spinach-baked-with-cake-dough', 'green-olives', 'brazil-nut', 'eggplant-caviar', 'bread-pita', 'pasta-wholemeal', 'sauce-pesto', 'oil', 'couscous', 'sauce-roast', 'prosecco', 'crackers', 'bread-toast', 'shrimp-prawn-small', 'panna-cotta', 'romanesco', 'water-with-lemon-juice', 'espresso-with-caffeine', 'egg-scrambled-prepared', 'juice-orange', 'ice-cubes', 'braided-white-loaf', 'emmental-cheese', 'croissant-wholegrain', 'hazelnut-chocolate-spread-nutella-ovomaltine-caotina', 'tomme', 'water-mineral', 'hazelnut', 'bacon-raw', 'bread-nut', 'black-forest-tart', 'soup-miso', 'peach', 'figs', 'beef-filet', 'mustard-dijon', 'rice-basmati', 'mashed-potatoes-prepared-with-full-fat-milk-with-butter', 'dumplings', 'pumpkin', 'swiss-chard', 'red-cabbage', 'spinach-raw', 'naan-indien-bread', 'chicken-curry-cream-coconut-milk-curry-spices-paste', 'crunch-muesli', 'biscuits', 'bread-french-white-flour', 'meatloaf', 'fresh-cheese', 'honey', 'vegetable-mix-peas-and-carrots', 'parsley', 'brownie', 'dairy-ice-cream', 'tea-black', 'carrot-cake', 'fish-fingers-breaded', 'salad-dressing', 'dried-meat', 'chicken-breast', 'mixed-salad-chopped-without-sauce', 'feta', 'praline', 'tea-peppermint', 'walnut', 'potato-salad-with-mayonnaise-yogurt-dressing', 'kebab-in-pita-bread', 'kolhrabi', 'alfa-sprouts', 'brussel-sprouts', 'bacon-cooking', 'gruyere', 'bulgur', 'grapes', 'pork-escalope', 'chocolate-egg-small', 'cappuccino', 'zucchini-stewed-without-addition-of-fat-without-addition-of-salt', 'crisp-bread-wasa', 'bread-black', 'perch-fillets-lake', 'rosti', 'mango', 'sandwich-ham-cheese-and-butter', 'muesli', 'spinach-steamed-without-addition-of-salt', 'fish', 'risotto-without-cheese-cooked', 'milk-chocolate-with-hazelnuts', 'cake-oblong', 'crisps', 'pork', 'pomegranate', 'sweet-corn-canned', 'flakes-oat', 'greek-salad', 'cantonese-fried-rice', 'sesame-seeds', 'bouillon', 'baked-potato', 'fennel', 'meat', 'bread-olive', 'croutons', 'philadelphia', 'mushroom-average-stewed-without-addition-of-fat-without-addition-of-salt', 'bell-pepper-red-stewed-without-addition-of-fat-without-addition-of-salt', 'white-chocolate', 'mixed-nuts', 'breadcrumbs-unspiced', 'fondue', 'sauce-mushroom', 'tea-spice', 'strawberries', 'tea-rooibos', 'pie-plum-baked-with-cake-dough', 'potatoes-au-gratin-dauphinois-prepared', 'capers', 'vegetables', 'bread-wholemeal-toast', 'red-radish', 'fruit-tart', 'beans-kidney', 'sauerkraut', 'mustard', 'country-fries', 'ketchup', 'pasta-linguini-parpadelle-tagliatelle', 'chicken-cut-into-stripes-only-meat', 'cookies', 'sun-dried-tomatoe', 'bread-ticino', 'semi-hard-cheese', 'margarine', 'porridge-prepared-with-partially-skimmed-milk', 'soya-drink-soy-milk', 'juice-multifruit', 'popcorn-salted', 'chocolate-filled', 'milk-chocolate', 'bread-fruit', 'mix-of-dried-fruits-and-nuts', 'corn', 'tete-de-moine', 'dates', 'pistachio', 'celery', 'white-radish', 'oat-milk', 'cream-cheese', 'bread-rye', 'witloof-chicory', 'apple-crumble', 'goat-cheese-soft', 'grapefruit-pomelo', 'risotto-with-mushrooms-cooked', 'blue-mould-cheese', 'biscuit-with-butter', 'guacamole', 'pecan-nut', 'tofu', 'cordon-bleu-from-pork-schnitzel-fried', 'paprika-chips', 'quinoa', 'kefir-drink', 'm-m-s', 'salad-rocket', 'bread-spelt', 'pizza-with-ham-with-mushrooms-baked', 'fruit-coulis', 'plums', 'beef-minced-only-meat', 'pizza-with-ham-baked', 'pineapple', 'soup-tomato', 'cheddar', 'tea-fruit', 'rice-jasmin', 'seeds', 'focaccia', 'milk', 'coleslaw-chopped-without-sauce', 'pastry-flaky', 'curd', 'savoury-puff-pastry-stick', 'sweet-potato', 'chicken-leg', 'croissant', 'sour-cream', 'ham-turkey', 'processed-cheese', 'fruit-compotes', 'cheesecake', 'pasta-tortelloni-stuffing', 'sauce-cocktail', 'croissant-with-chocolate-filling', 'pumpkin-seeds', 'artichoke', 'champagne', 'grissini', 'sweets-candies', 'brie', 'wienerli-swiss-sausage', 'syrup-diluted-ready-to-drink', 'apple-pie', 'white-bread-with-butter-eggs-and-milk', 'savoury-puff-pastry', 'anchovies', 'tuna-in-oil-drained', 'lemon-pie', 'meat-terrine-pate', 'coriander', 'falafel-balls', 'berries', 'latte-macchiato-with-caffeine', 'faux-mage-cashew-vegan-chers', 'beans-white', 'sugar-melon', 'mixed-seeds', 'hamburger', 'hamburger-bun', 'oil-vinegar-salad-dressing', 'soya-yaourt-yahourt-yogourt-ou-yoghourt', 'chocolate-milk-chocolate-drink', 'celeriac', 'chocolate-mousse', 'cenovis-yeast-spread', 'thickened-cream-35', 'meringue', 'lamb-chop', 'shrimp-prawn-large', 'beef', 'lemon', 'croque-monsieur', 'chives', 'chocolate-cookies', 'birchermuesli-prepared-no-sugar-added', 'fish-crunchies-battered', 'muffin', 'savoy-cabbage-steamed-without-addition-of-salt', 'pine-nuts', 'chorizo', 'chia-grains', 'frying-sausage', 'french-pizza-from-alsace-baked', 'chocolate', 'cooked-sausage', 'grits-polenta-maize-flour', 'gummi-bears-fruit-jellies-jelly-babies-with-fruit-essence', 'wine-rose', 'coca-cola', 'raspberries', 'roll-with-pieces-of-chocolate', 'goat-average-raw', 'lemon-cake', 'coconut-milk', 'rice-wild', 'gluten-free-bread', 'pearl-onions', 'buckwheat-pancake', 'bread-5-grain', 'light-beer', 'sugar-glazing', 'tzatziki', 'butter-herb', 'ham-croissant', 'corn-crisps', 'lentils-green-du-puy-du-berry', 'cocktail', 'rice-whole-grain', 'veal-sausage', 'cervelat', 'sorbet', 'aperitif-with-alcohol-aperol-spritz', 'dips', 'corn-flakes', 'peas', 'tiramisu', 'apricots', 'cake-marble', 'lamb', 'lasagne-meat-prepared', 'coca-cola-zero', 'cake-salted', 'dough-puff-pastry-shortcrust-bread-pizza-dough', 'rice-waffels', 'sekt', 'brioche', 'vegetable-au-gratin-baked', 'mango-dried', 'processed-meat-charcuterie', 'mousse', 'sauce-sweet-sour', 'basil', 'butter-spread-puree-almond', 'pie-apricot-baked-with-cake-dough', 'rusk-wholemeal', 'beef-roast', 'vanille-cream-cooked-custard-creme-dessert', 'pasta-in-conch-form', 'nuts', 'sauce-carbonara', 'fig-dried', 'pasta-in-butterfly-form-farfalle', 'minced-meat', 'carrot-steamed-without-addition-of-salt', 'ebly', 'damson-plum', 'shoots', 'bouquet-garni', 'coconut', 'banana-cake', 'waffle', 'apricot-dried', 'sauce-curry', 'watermelon-fresh', 'sauce-sweet-salted-asian', 'pork-roast', 'blackberry', 'smoked-cooked-sausage-of-pork-and-beef-meat-sausag', 'bean-seeds', 'italian-salad-dressing', 'white-asparagus', 'pie-rhubarb-baked-with-cake-dough', 'tomato-stewed-without-addition-of-fat-without-addition-of-salt', 'cherries', 'nectarine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = {}\n",
    "args['num_classes'] = len(CLASSES)\n",
    "args['input_size'] = 224\n",
    "args['num_workers'] = 0\n",
    "args['num_queries'] = 100\n",
    "args['null_class_coef'] = 0.5\n",
    "args['bs'] = 8\n",
    "args['lr'] = 1e-3\n",
    "args['epochs'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_transforms(is_train=True):\n",
    "    if is_train:\n",
    "        return A.Compose([\n",
    "                        A.OneOf([\n",
    "                            A.HueSaturationValue(\n",
    "                                hue_shift_limit=0.2, \n",
    "                                sat_shift_limit= 0.2, \n",
    "                                val_shift_limit=0.2, \n",
    "                                p=0.9),\n",
    "                            A.RandomBrightnessContrast(\n",
    "                                brightness_limit=0.2, \n",
    "                                contrast_limit=0.2, \n",
    "                                p=0.9)]\n",
    "                            ,p=0.9),\n",
    "            \n",
    "                        A.ToGray(p=0.01),\n",
    "                    \n",
    "                        A.HorizontalFlip(p=0.2),\n",
    "                      \n",
    "                        A.VerticalFlip(p=0.2),\n",
    "\n",
    "                        A.Resize(height=args['input_size'], width=args['input_size'], p=1),\n",
    "\n",
    "                        A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=0, p=0.5),\n",
    "                      \n",
    "                        ToTensorV2(p=1.0)],\n",
    "                      \n",
    "                      p=1.0,\n",
    "                     \n",
    "                      bbox_params=A.BboxParams(format='coco',\n",
    "                                               min_area=0, \n",
    "                                               min_visibility=0,\n",
    "                                               label_fields=['labels'])\n",
    "                      )\n",
    "    else:\n",
    "        return A.Compose([A.Resize(height=args['input_size'], \n",
    "                                   width=args['input_size'], \n",
    "                                   p=1.0),\n",
    "                      ToTensorV2(p=1.0)], \n",
    "                      p=1.0, \n",
    "                      bbox_params=A.BboxParams(format='coco',\n",
    "                                               min_area=0, \n",
    "                                               min_visibility=0,\n",
    "                                               label_fields=['labels'])\n",
    "                      )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reading the annotation files\n",
    "with open(TRAIN_ANNOTATIONS_PATH) as f:\n",
    "    train_annotations_data = json.load(f)\n",
    "    \n",
    "with open(VAL_ANNOTATIONS_PATH) as f:\n",
    "    val_annotations_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_label_id(cats):\n",
    "    return [CLASSES.index(item) for item in cats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ifnone(a, b): return b if a is None else a\n",
    "def get_annotations(fname, prefix=None):\n",
    "    \"Open a COCO style json in `fname` and returns the lists of filenames (with maybe `prefix`) and labelled bboxes.\"\n",
    "    annot_dict = json.load(open(fname))\n",
    "    id2images, id2bboxes, id2cats = {}, collections.defaultdict(list), collections.defaultdict(list)\n",
    "    classes = {o['id']:o['name'] for o in annot_dict['categories']}\n",
    "    for o in annot_dict['annotations']:\n",
    "        bb = o['bbox']\n",
    "        id2bboxes[o['image_id']].append([bb[0],bb[1], bb[2], bb[3]])\n",
    "        id2cats[o['image_id']].append(classes[o['category_id']])\n",
    "    id2images = {o['id']:ifnone(prefix, '') + o['file_name'] for o in annot_dict['images'] if o['id'] in id2bboxes}\n",
    "    ids = list(id2images.keys())\n",
    "    return [id2images[k] for k in ids], [id2bboxes[k] for k in ids], [get_label_id(id2cats[k]) for k in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imgs_train, bboxes_train, cat_train = get_annotations(TRAIN_ANNOTATIONS_PATH) \n",
    "df_train = pd.DataFrame([])\n",
    "df_train['fnames'] = imgs_train\n",
    "df_train['bboxes'] = bboxes_train\n",
    "df_train['labels'] = cat_train\n",
    "\n",
    "imgs_valid, bboxes_valid, cat_valid = get_annotations(VAL_ANNOTATIONS_PATH) \n",
    "df_valid = pd.DataFrame([])\n",
    "df_valid['fnames'] = imgs_valid\n",
    "df_valid['bboxes'] = bboxes_valid\n",
    "df_valid['labels'] = cat_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fnames</th>\n",
       "      <th>bboxes</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>131094.jpg</td>\n",
       "      <td>[[39.5, 39.5, 167.0, 92.0], [8.2599609375, 8.2...</td>\n",
       "      <td>[5, 6, 7, 0, 8, 9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>131097.jpg</td>\n",
       "      <td>[[44.47625, 44.47625, 311.3225, 257.0825], [80...</td>\n",
       "      <td>[11, 12, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>131098.jpg</td>\n",
       "      <td>[[58.161249999999995, 58.161249999999995, 256....</td>\n",
       "      <td>[12, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>131100.jpg</td>\n",
       "      <td>[[56.68375000000002, 76.73375, 288.36249999999...</td>\n",
       "      <td>[12, 14, 15, 5, 16]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>131101.jpg</td>\n",
       "      <td>[[122.5, 122.5, 195.0, 128.0]]</td>\n",
       "      <td>[17]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fnames                                             bboxes  \\\n",
       "0  131094.jpg  [[39.5, 39.5, 167.0, 92.0], [8.2599609375, 8.2...   \n",
       "1  131097.jpg  [[44.47625, 44.47625, 311.3225, 257.0825], [80...   \n",
       "2  131098.jpg  [[58.161249999999995, 58.161249999999995, 256....   \n",
       "3  131100.jpg  [[56.68375000000002, 76.73375, 288.36249999999...   \n",
       "4  131101.jpg                     [[122.5, 122.5, 195.0, 128.0]]   \n",
       "\n",
       "                labels  \n",
       "0   [5, 6, 7, 0, 8, 9]  \n",
       "1          [11, 12, 5]  \n",
       "2              [12, 5]  \n",
       "3  [12, 14, 15, 5, 16]  \n",
       "4                 [17]  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FoodDataset(Dataset):\n",
    "    def __init__(self, path, dataframe, transforms=None):\n",
    "        self.path = path\n",
    "        self.dataframe = dataframe\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        image_id = self.dataframe['fnames'].iloc[index]\n",
    "        image_path = os.path.join(self.path, image_id)\n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = np.array(image).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        # DETR takes in data in coco format \n",
    "        boxes = self.dataframe['bboxes'].iloc[index]\n",
    "        \n",
    "        \n",
    "        boxes = np.array(boxes)\n",
    "        #Area of bb\n",
    "        area = boxes[:,2] * boxes[:,3]\n",
    "#         print(area)\n",
    "        area = torch.as_tensor(area, dtype=torch.float32)\n",
    "        # AS pointed out by PRVI It works better if the main class is labelled as zero\n",
    "        labels =  self.dataframe['labels'].iloc[index]\n",
    "        try:\n",
    "            if self.transforms:\n",
    "                sample = {\n",
    "                    'image': image,\n",
    "                    'bboxes': boxes,\n",
    "                    'labels': labels\n",
    "                }\n",
    "                sample = self.transforms(**sample)\n",
    "                image = sample['image']\n",
    "                boxes = sample['bboxes']\n",
    "                labels = sample['labels']\n",
    "\n",
    "\n",
    "            #Normalizing BBOXES\n",
    "\n",
    "            _,h,w = image.shape\n",
    "            boxes = A.augmentations.bbox_utils.normalize_bboxes(sample['bboxes'],rows=h,cols=w)\n",
    "            target = {}\n",
    "\n",
    "            target['boxes'] = torch.as_tensor(boxes,dtype=torch.float32)\n",
    "            target['labels'] = torch.as_tensor(labels,dtype=torch.long)\n",
    "            target['image_id'] = torch.tensor([index])\n",
    "            target['area'] = area\n",
    "\n",
    "            image_id = image_id.split('.')[0]\n",
    "            image = image.type(torch.FloatTensor)\n",
    "            return image, target, image_id\n",
    "        except Exception as e:\n",
    "            # return image_id\n",
    "            # print(str(e))\n",
    "            print(image_id)\n",
    "            return image, '_', image_id \n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../albumentations\\albumentations\\augmentations\\dropout\\cutout.py:52: FutureWarning: Cutout has been deprecated. Please use CoarseDropout\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "ds_train = FoodDataset(TRAIN_IMAGE_DIRECTIORY, df_train, transforms = get_transforms(True))\n",
    "ds_valid = FoodDataset(VAL_IMAGE_DIRECTIORY, df_valid, transforms = get_transforms(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dl_train = DataLoader(\n",
    "    ds_train,\n",
    "    batch_size = args['bs'],\n",
    "    shuffle=False,\n",
    "    num_workers=args['num_workers'],\n",
    "    collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "dl_valid = DataLoader(\n",
    "    ds_valid,\n",
    "    batch_size = args['bs'],\n",
    "    shuffle=False,\n",
    "    num_workers=args['num_workers'],\n",
    "    collate_fn=collate_fn\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 4995/4995 [49:16<00:00,  1.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# list_error = []\n",
    "for (x,t,i) in tqdm(dl_train):\n",
    "    # print(i)\n",
    "    # list_error.append(i)\n",
    "    continue\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (668683560.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_6996\\668683560.py\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (x,t,i) in tqdm(dl_valid):\n",
    "    # print(i)\n",
    "    # list_error.append(i)\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DETRModel(nn.Module):\n",
    "    def __init__(self,num_classes,num_queries):\n",
    "        super(DETRModel,self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_queries = num_queries\n",
    "        \n",
    "        self.model = torch.hub.load('facebookresearch/detr', 'detr_resnet50', pretrained=True)\n",
    "        self.in_features = self.model.class_embed.in_features\n",
    "        \n",
    "        self.model.class_embed = nn.Linear(in_features=self.in_features,out_features=self.num_classes)\n",
    "        self.model.num_queries = self.num_queries\n",
    "        \n",
    "    def forward(self,images):\n",
    "        return self.model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = HungarianMatcher()\n",
    "\n",
    "weight_dict = weight_dict = {'loss_ce': 1., 'loss_bbox': 1., 'loss_giou': 1.}\n",
    "\n",
    "losses = ['labels', 'boxes', 'cardinality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one(data_loader,model,criterion,optimizer, args, device,scheduler,epoch):\n",
    "    model.train()\n",
    "    criterion.train()\n",
    "    \n",
    "    summary_loss = AverageMeter()\n",
    "    \n",
    "    tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "    \n",
    "    for step, (images, targets, image_ids) in enumerate(tk0):\n",
    "        \n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "\n",
    "        output = model(images)\n",
    "        \n",
    "        loss_dict = criterion(output, targets)\n",
    "        weight_dict = criterion.weight_dict\n",
    "        \n",
    "        losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        summary_loss.update(losses.item(),args['bs'])\n",
    "        tk0.set_postfix(loss=summary_loss.avg)\n",
    "        \n",
    "    return summary_loss\n",
    "\n",
    "def eval_one(data_loader, model,criterion, args, device):\n",
    "    model.eval()\n",
    "    criterion.eval()\n",
    "    summary_loss = AverageMeter()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "        for step, (images, targets, image_ids) in enumerate(tk0):\n",
    "            \n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            output = model(images)\n",
    "        \n",
    "            loss_dict = criterion(output, targets)\n",
    "            weight_dict = criterion.weight_dict\n",
    "        \n",
    "            losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "            \n",
    "            summary_loss.update(losses.item(), args['bs'])\n",
    "            tk0.set_postfix(loss=summary_loss.avg)\n",
    "    \n",
    "    return summary_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "model = DETRModel(num_classes=args['num_classes'],num_queries=args['num_queries'])\n",
    "model = model.to(device)\n",
    "criterion = SetCriterion(args['num_classes']-1, matcher, weight_dict, eos_coef = args['null_class_coef'], losses=losses)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=args['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(list([item.to(device) for item in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in range(args['epochs']):\n",
    "    train_loss = train_one(dl_train, model,criterion, optimizer,args, device,scheduler=None,epoch=epoch)\n",
    "    valid_loss = eval_one(dl_valid, model,criterion, args, device)\n",
    "\n",
    "    print('|EPOCH {}| TRAIN_LOSS {}| VALID_LOSS {}|'.format(epoch+1,train_loss.avg,valid_loss.avg))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "det",
   "language": "python",
   "name": "det"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
